---
description: 神经概率语言模型
---

# 4、论文 A Neural Probabilistic Language Model

## Abstract

在摘要中作者提到，通过学习一个分布式的词表示来克服维数的诅咒，它允许每个训练句子向模型告知一个指数数量的语义相邻句子。该模型同时学习 (1) 每个单词的分布式表示，以及 (2) 用这些表示 表示的单词序列的概率函数。

## Introduction

在介绍中，作者举例到，如果相对10000个词建立连续10个词得联合分布，那么可能需要得参数是$$10000^{10}-1$$，当对连续变量进行建模时，我们比较容易获得泛化（光滑得函数类，神经网路模型，高斯模型等），对于离散空间泛化结构就不明显了，每个离散变量得取值很大时，大多观测对象在汉明距离上几乎是最大得。

本文提出在高维情况下，重要的是将概率质量均匀分布在每个训练点周围的各个方向上。这里提出得泛化方式也与之前最先进得统计方法模型不同。

语言得统计模型公式如下$$P(w_1^T)=\prod_{t=1}^T(P(w_t|w_{l}^{t-1})$$，我们可以看出，每次计算t得概率时，都需要从1\~t-1的概率，这无形中就增加了很大的计算量，本文利用统计上词序更加依赖暂时距离较近的词这一事实，因此提出了n-gram模型结构，计算公式如下$$P(w_t|w_1^{t-1})=P(w_t|w_{t-n+1}^{t-1})$$。

在文本中总会出现上下文连续的词但预料中没有出现的情况，一种简单的解决办法就是将n回退到三元模型。从本质上说一个新的单词序列是通过“粘合”非常短的长度为1,2…或者在训练数据中经常出现的单词上。获取下一个片段概率的规则隐含在后退或插值n-gram算法的细节中。研究中一般采用3，但显然，单词前面的序列中又更多的信息需要预测，并不仅仅时单词前的两个单词。因此该方法至少需要两点需要改进

* 它不考虑超过1到2个单词的上下文
* 不考虑单词之间的“相似性”

